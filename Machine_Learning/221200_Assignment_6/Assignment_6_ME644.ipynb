{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gfpvneI0SaH",
    "outputId": "ed7d1280-22d9-4104-a55f-bf3c5cdc1190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 6813.198923800566\n",
      "Loss at epoch 500: 5227.560374596914\n",
      "Loss at epoch 1000: 3307.0279562029878\n",
      "Loss at epoch 1500: 2497.3077752902627\n",
      "Loss at epoch 2000: 1937.0424157439243\n",
      "Loss at epoch 2500: 1471.0927818997475\n",
      "Loss at epoch 3000: 1110.2182296118324\n",
      "Loss at epoch 3500: 835.3470685208082\n",
      "Loss at epoch 4000: 611.198178453714\n",
      "Loss at epoch 4500: 414.1910153612584\n",
      "Loss at epoch 5000: 270.56313500211115\n",
      "Loss at epoch 5500: 179.62750004725953\n",
      "Loss at epoch 6000: 117.64071976368625\n",
      "Loss at epoch 6500: 78.8757559017861\n",
      "Loss at epoch 7000: 52.457045067054985\n",
      "Loss at epoch 7500: 34.63452614618048\n",
      "Loss at epoch 8000: 22.47687123300391\n",
      "Loss at epoch 8500: 14.455519475418054\n",
      "Loss at epoch 9000: 9.648347216702637\n",
      "Loss at epoch 9500: 7.382303499898655\n",
      "Loss at epoch 10000: 6.099470626721984\n",
      "Loss at epoch 10500: 5.481470212410995\n",
      "Loss at epoch 11000: 4.906118897966373\n",
      "Loss at epoch 11500: 4.364433520270035\n",
      "Loss at epoch 12000: 4.154656421786957\n",
      "Loss at epoch 12500: 4.12115353173036\n",
      "Loss at epoch 13000: 4.1024124329012395\n",
      "Loss at epoch 13500: 4.096251748974442\n",
      "Loss at epoch 14000: 4.091696676503011\n",
      "Loss at epoch 14500: 4.087209038951041\n",
      "Loss at epoch 15000: 4.084679679871291\n",
      "Loss at epoch 15500: 4.083254888844699\n",
      "Loss at epoch 16000: 4.080328133394424\n",
      "Loss at epoch 16500: 4.079048538111084\n",
      "Loss at epoch 17000: 4.0772947157347526\n",
      "Loss at epoch 17500: 4.075706551134622\n",
      "Loss at epoch 18000: 4.07437662052564\n",
      "Loss at epoch 18500: 4.072930877379906\n",
      "Loss at epoch 19000: 4.072295725651946\n",
      "Loss at epoch 19500: 4.071152745768803\n",
      "Loss at epoch 20000: 4.0699793769069075\n",
      "Loss at epoch 20500: 4.0695858962783165\n",
      "Loss at epoch 21000: 4.068528648596923\n",
      "Loss at epoch 21500: 4.0682436724421995\n",
      "Loss at epoch 22000: 4.0678516793708175\n",
      "Loss at epoch 22500: 4.0670938792959985\n",
      "Loss at epoch 23000: 4.066842693625616\n",
      "Loss at epoch 23500: 4.066584429764718\n",
      "Loss at epoch 24000: 4.065857989769509\n",
      "Loss at epoch 24500: 4.065572089191431\n",
      "Loss at epoch 25000: 4.064917173865446\n",
      "Mean Squared Error: 7.853414676554222\n",
      "Mean Absolute Error: 1.9456350246991116\n",
      "[[382.43599114]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# Data generation\n",
    "func = lambda x1, x2: (1 - x1) ** 2 + 100 * (x2 - x1 ** 2) ** 2\n",
    "X = [[rnd.uniform(-1, 1), rnd.uniform(-1, 1)] for _ in range(200)]\n",
    "T1 = [[rnd.uniform(-1, 1), rnd.uniform(-1, 1)] for _ in range(100)]\n",
    "Y = [func(x[0], x[1]) for x in X]\n",
    "Y_test = [func(t[0], t[1]) for t in T1]\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).reshape(-1, 1)\n",
    "T1 = np.array(T1)\n",
    "Y_T = np.array(Y_test).reshape(-1, 1)\n",
    "\n",
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def absolute(z):\n",
    "    return np.abs(z)\n",
    "\n",
    "# Forward propagation with ReLU for hidden layer and absolute activation for output\n",
    "def forward_propagation(X):\n",
    "    u = np.dot(X, weights_input_to_hidden) + biases_hidden\n",
    "    z = relu(u)  # Hidden layer activation\n",
    "    v = np.dot(z, weights_hidden_to_output) + biases_output\n",
    "    yhat = relu(v)  # Absolute activation for output layer\n",
    "    return z, yhat\n",
    "\n",
    "#clip gradients\n",
    "def clip_gradients(dw_hidden, db_hidden, dw_output, db_output, clip_value=1.0):\n",
    "    np.clip(dw_hidden, -clip_value, clip_value, out=dw_hidden)\n",
    "    np.clip(db_hidden, -clip_value, clip_value, out=db_hidden)\n",
    "    np.clip(dw_output, -clip_value, clip_value, out=dw_output)\n",
    "    np.clip(db_output, -clip_value, clip_value, out=db_output)\n",
    "\n",
    "\n",
    "# Backpropagation with gradient calculations\n",
    "def backpropagation(X, Y, z, yhat):\n",
    "    e = yhat - Y\n",
    "    dw_output = np.dot(z.T, e)\n",
    "    db_output = np.sum(e, axis=0, keepdims=True)\n",
    "    dz_hidden = np.dot(e, weights_hidden_to_output.T) * relu_derivative(z)\n",
    "    dw_hidden = np.dot(X.T, dz_hidden)\n",
    "    db_hidden = np.sum(dz_hidden, axis=0, keepdims=True)\n",
    "    return dw_hidden, db_hidden, dw_output, db_output\n",
    "\n",
    "\n",
    "\n",
    "# Update parameters with gradient clipping\n",
    "def update_parameters(dw_hidden, db_hidden, dw_output, db_output, learning_rate):\n",
    "    clip_gradients(dw_hidden, db_hidden, dw_output, db_output)\n",
    "    global weights_input_to_hidden, biases_hidden, weights_hidden_to_output, biases_output\n",
    "    weights_input_to_hidden -= learning_rate * dw_hidden\n",
    "    biases_hidden -= learning_rate * db_hidden\n",
    "    weights_hidden_to_output -= learning_rate * dw_output\n",
    "    biases_output -= learning_rate * db_output\n",
    "\n",
    "# Training function\n",
    "def train(X, Y, epochs, learning_rate, epoch_print):\n",
    "    for epoch in range(epochs):\n",
    "        z, yhat = forward_propagation(X)\n",
    "        loss = np.mean((yhat - Y) ** 2)  # Mean squared error\n",
    "        dw_hidden, db_hidden, dw_output, db_output = backpropagation(X, Y, z, yhat)\n",
    "        update_parameters(dw_hidden, db_hidden, dw_output, db_output, learning_rate)\n",
    "        #if epoch % 1000 == 0:\n",
    "         #   learning_rate *= 0.99  # Optional: Decay learning rate\n",
    "        if epoch % epoch_print == 0:\n",
    "            print(f\"Loss at epoch {epoch}: {loss}\")\n",
    "\n",
    "# Prediction function\n",
    "def predict(X):\n",
    "    z, yhat = forward_propagation(X)\n",
    "    mse = np.mean((yhat - Y_T) ** 2)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    mae = np.mean(np.abs(yhat - Y_T))\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# Initialize parameters\n",
    "input_size = 2\n",
    "hidden_size = 70\n",
    "output_size = 1\n",
    "weights_input_to_hidden = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "weights_hidden_to_output = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "biases_hidden = np.random.randn(1, hidden_size)\n",
    "biases_output = np.random.randn(1, output_size)\n",
    "\n",
    "# Training and Prediction\n",
    "epochs = 25001\n",
    "learning_rate = 0.0008\n",
    "epoch_print = 500\n",
    "train(X, Y, epochs, learning_rate, epoch_print)\n",
    "predict(T1)\n",
    "\n",
    "print(forward_propagation([-1,-1])[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
